{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating AutoML Tables training and deployment with Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample demonostrates how to implement a Kubeflow Pipelines pipeline that orchestrates BigQuery and AutoML Tables.\n",
    "\n",
    "The scenario used in the lab is  predicting customer lifetime value (CLV).\n",
    "\n",
    "The goal of CLV modeling is to identify the most valuable customers - customers that are going to generate the highest value in a given future time range. The CLV models are built from a variety of data sources - historical sales data being the most important one and in many cases the only one. \n",
    "\n",
    "Predicting Customer Lifetime Value (CLV)  is a representative example of a use case where you may need to fine-tune and re-train a predictive model on a frequent basis. As there is a constant flow of new sales transactions that constitute the core of training data, models have to be kept up to date with evolving purchase patterns. Automation of model training and deployment is critical. \n",
    "\n",
    "In the CLV model developed in this lab, the historical sales transactions are preprocessed and aggregated to engineer a set of latent features  representing the so-called RFM characteristics of your customers:\n",
    "- Recency: How active have they been recently?\n",
    "- Frequency: How often do they buy?\n",
    "- Monetary: What amount do they spend?\n",
    "\n",
    "The following diagram shows a succession of past sales for a set of four customers.\n",
    "\n",
    "![clv_timeline](https://github.com/jarokaz/mlops-labs/raw/master/images/clv-timeline.png)\n",
    "\n",
    "The diagram illustrates the RFM values for the customers, showing for each customer:\n",
    "- Recency: The time between the last purchase and today, represented by the distance between the leftmost circle and the vertical dotted line that's labeled **Now**.\n",
    "- Frequency: The time between purchaes, represented by the distance between the circles on a single line.\n",
    "- Monetary: The amount of money spent on each purchase, represented by the size of the circle.\n",
    "\n",
    "As demonstrated in the lab you usually create multiple features per each characteristic. For example, in the lab, Recency is captured by two features: *recency* and *T*.\n",
    "\n",
    "The RFM input features and the target label are engineered using the following process:\n",
    "- A time series of of historical sales transactions for a given customer is divided into two time periods: *the features period* and *the predict period*. A point in time that is used to divide the time series is referred two as *the threshold date*. \n",
    "- The transactions in *the features period* are aggregated to create the latent RFM input features \n",
    "- The transactions in *the predict period* are aggregated to calculate the target label representing the expected total value of the customer\n",
    "\n",
    "This process results in a single example per customer and a set of examples across a customer population constitutes a training set.\n",
    "\n",
    "The pipeline implemented in the lab, uses BigQuery as a source of historical sales transactions. BigQuery is also used to engineer RFM features. The model is then trained and deployed using AutoML Tables. The below diagram represents the control and data flow implemented by the pipeline.\n",
    "\n",
    "\n",
    "![Training pipeline](https://github.com/jarokaz/mlops-labs/raw/master/images/clv_train.png)\n",
    "\n",
    "1. The BQ query is run to process sales transactions in the *transactions* table into RFM features in the *features* table. \n",
    "1. The data from the *features* table is imported into the AutoML dataset\n",
    "1. The AutoML model is trained on the imported dataset\n",
    "1. After the training completes the evaluation metrics are retrieved and compared against the performance threshold\n",
    "1. If the model performs better than the threshold the model is deployed to AutoML Deployment\n",
    "\n",
    "The sample dataset used in the lab is based on the publicaly available [Online Retail Data Set](http://archive.ics.uci.edu/ml/datasets/Online+Retail) from the UCI Machine Learning Repository. The original dataset was preprocessed to conform to the following schema:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| customer_id | string | A unique customer ID |\n",
    "| order_date | date (yyyy-MM-dd) | The date of a transaction. Transactions (potentially from multiple invoices) are grouped by day |\n",
    "| quantity | integer | A number of items of a single SKU in a transaction |\n",
    "| unit_price | float | A unit price of a SKU |\n",
    "\n",
    "The feature engineering query generates the features table with the below schema. \n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| monetary | Float | The total spend by a customer in the features period|\n",
    "| frequency | Integer | The number of transactions placed by a customer in the features period |\n",
    "| recency | Integer |  The time (in days) between the first and the last orders in the features period |\n",
    "| T | Integer | The time between the first order placed and in the threshold date|\n",
    "| time_between | Float |  The average time betwee orders in the features period |\n",
    "| avg_basket_value | Float |  The averate monetary value of the customer's basket in the features period |\n",
    "| avg_basket_size | Float |  The average number of items in a basket in the features perio|\n",
    "| cnt_returns | Integer |  The number of returns in the features period|\n",
    "| target_monetary | Float | The total amount spent in the predict period. This is the label for predictions|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment\n",
    "Let's start with configuring your GCP environment settings and uploading the sales transactions into BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'mlops-dev-env'\n",
    "ENDPOINT = '55c0912bf09eab62-dot-us-central2.pipelines.googleusercontent.com'\n",
    "DATASET_ID = 'clv'\n",
    "DATASET_LOCATION = 'US'\n",
    "TRANSACTIONS_TABLE_ID = 'transactions'\n",
    "TRANSACTIONS_TABLE_SCHEMA = 'customer_id:STRING,order_date:DATE,quantity:INTEGER,unit_price:FLOAT'\n",
    "TRANSACTIONS_SOURCE_FILE = 'transactions.csv'\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.5.1/components/gcp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a BigQuery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sale transactions data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TRANSACTIONS_TABLE_ID \\\n",
    "$TRANSACTIONS_SOURCE_FILE \\\n",
    "$TRANSACTIONS_TABLE_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset\n",
    "To query data in BigQuery you can use BigQuery Python client library ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM `{{ source_table }}`\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TRANSACTIONS_TABLE_ID))\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are multiple sales transactions per customer. They represent the purchasing history and behavior of a given customer. For example, \n",
    "the customer identified by 16553 has 85 orders. Most of them are new purchases. Some of them are returns - the records with a negative quantity.\n",
    "\n",
    "The feature engineering query converts these 85 records into a single record representing the RFM charateristics of this customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the KFP training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create component factories for the pre-defined GCP components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None,\n",
    "    url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base docker image for the custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.7\n",
    "RUN pip3 install --upgrade google-cloud-bigquery google-api-core google-cloud-automl grpcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"clv_components\"\n",
    "IMAGE_URI=\"gcr.io/{}/{}:latest\".format(PROJECT_ID, IMAGE_NAME)\n",
    "!gcloud builds submit --timeout 15m --tag $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create AutoML dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def automl_create_dataset(project_id: str, region:str, display_name:str) -> NamedTuple(\n",
    "#    'Outputs', [('dataset_name', str)]):\n",
    "def automl_create_dataset(project_id: str, region:str, display_name:str) -> str:\n",
    "    import logging\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient(project=project_id, region=region)\n",
    "    \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_display_name=display_name\n",
    "    )\n",
    "    \n",
    "    logging.info('Created dataset: {}'.format(dataset.name))\n",
    "    \n",
    "    return dataset.name\n",
    "\n",
    "automl_create_dataset_op = func_to_container_op(automl_create_dataset, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_import_data_from_bq(dataset_name:str, table_uri:str) -> str:\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient()\n",
    "    \n",
    "    import_data_response = client.import_data(\n",
    "        dataset_name = dataset_name,\n",
    "        bigquery_input_uri = table_uri\n",
    "    )\n",
    "    logging.info('Import operation started: {}'.format(import_data_response.operation))\n",
    "    logging.info('Import operation completed: {}'.format(import_data_response.result()))\n",
    "    \n",
    "    return dataset_name\n",
    "\n",
    "automl_import_data_from_bq_op = func_to_container_op(automl_import_data_from_bq, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_set_target_column(dataset_name:str, target_column_name:str) -> str:\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient()\n",
    "    \n",
    "    logging.info('Setting target column to: {}'.format(target_column_name))\n",
    "    \n",
    "    import_data_response = client.set_target_column(\n",
    "        dataset_name = dataset_name,\n",
    "        column_spec_display_name = target_column_name\n",
    "    )\n",
    "    \n",
    "    return dataset_name\n",
    "\n",
    "automl_set_target_column_op = func_to_container_op(automl_set_target_column, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_create_model(\n",
    "        project_id: str,\n",
    "        region: str,\n",
    "        display_name: str,\n",
    "        dataset_name: str,\n",
    "        optimization_objective: str,\n",
    "        train_budget_milli_node_hours: int\n",
    "    ) -> str:\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient(project=project_id, region=region)\n",
    "    \n",
    "    create_model_response = client.create_model(\n",
    "        model_display_name=display_name,\n",
    "        dataset_name=dataset_name,\n",
    "        train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "        optimization_objective=optimization_objective\n",
    "    )\n",
    "    logging.info('Create model operation started: {}'.format(create_model_response.operation))\n",
    "    model = create_model_response.result()\n",
    "    logging.info('Create model operation completed: {}'.format(model.name))\n",
    "    \n",
    "    return model.name\n",
    "\n",
    "\n",
    "automl_create_model__op = func_to_container_op(automl_create_model, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve and log AutoML evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_log_regression_metrics(model_path: str,\n",
    "               primary_metric:str) -> NamedTuple('Outputs', [('primary_metric', str), ('primary_metric_value', float)]):\n",
    "    \n",
    "    import logging\n",
    "    import json\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient()\n",
    "\n",
    "    # Retrieve evaluation metrics\n",
    "    for evaluation in client.list_model_evaluations(model_name=model_path):\n",
    "        if evaluation.regression_evaluation_metrics.ListFields():\n",
    "            evaluation_metrics = evaluation.regression_evaluation_metrics      \n",
    "    primary_metric_value = getattr(evaluation_metrics, primary_metric)\n",
    "    \n",
    "    # Write the primary metric as a KFP pipeline metric\n",
    "    metrics = {\n",
    "        'metrics': [{\n",
    "            'name': primary_metric.replace('_', '-'),\n",
    "            'numberValue': primary_metric_value\n",
    "        }]\n",
    "    }\n",
    "    with open('/mlpipeline-metrics.json', 'w') as f:\n",
    "       json.dump(metrics, f)\n",
    "    \n",
    "    return (primary_metric, primary_metric_value)\n",
    "    \n",
    "    \n",
    "log_regression_metrics_op = func_to_container_op(automl_log_regression_metrics, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_deploy_model(model_path: str):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    from google.cloud.automl_v1beta1 import enums\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient()\n",
    "    \n",
    "    model = client.get_model(model_name=model_path)\n",
    "    if model.deployment_state != enums.Model.DeploymentState.DEPLOYED:\n",
    "        logging.info(\"Starting model deployment: {}\".format(model_path))\n",
    "        response = client.deploy_model(model_name=model_path)\n",
    "        response.result() # Wait for operation to complete\n",
    "        logging.info(\"Deployment completed\")\n",
    "    else:\n",
    "         logging.info(\"Model already deployed\")\n",
    "    \n",
    "    \n",
    "deploy_model_op = func_to_container_op(automl_deploy_model, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline\n",
    "#### Set default values for pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = PROJECT_ID\n",
    "features_dataset_id = DATASET_ID\n",
    "features_dataset_location = DATASET_LOCATION\n",
    "features_table_id = 'features'\n",
    "aml_compute_region = 'us-central1'\n",
    "aml_dataset_name = 'clv_features'\n",
    "aml_model_name = 'clv_regression'\n",
    "target_column_name = 'target_monetary'\n",
    "train_budget = 1000\n",
    "optimization_objective = 'MINIMIZE_MAE'\n",
    "primary_metric = 'mean_absolute_error'\n",
    "deployment_threshold = 900\n",
    "\n",
    "# Read and render the query template\n",
    "query_template_file = 'query_template.sql.jinja'\n",
    "with open(query_template_file, 'r') as file:\n",
    "    query_template = file.read()\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    data_source_id='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TRANSACTIONS_TABLE_ID),\n",
    "    threshold_date='2011-08-08',\n",
    "    predict_end='2011-12-12',\n",
    "    max_monetary=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='CLV Training',\n",
    "    description='CLV Training Pipeline using BigQuery for feature engineering and Automl Tables for model training'\n",
    ")\n",
    "def clv_train(\n",
    "    project_id:str,\n",
    "    feature_engineering_query:str =query,\n",
    "    aml_compute_region:str =aml_compute_region,\n",
    "    features_table_id:str =features_table_id,\n",
    "    features_dataset_id:str =features_dataset_id,\n",
    "    features_dataset_location:str =features_dataset_location,\n",
    "    aml_dataset_name:str =aml_dataset_name,\n",
    "    target_column_name:str =target_column_name,\n",
    "    aml_model_name:str =aml_model_name,\n",
    "    train_budget:'Integer' =train_budget,\n",
    "    optimization_objective:str =optimization_objective,\n",
    "    primary_metric:str =primary_metric,\n",
    "    deployment_threshold:'Float' =deployment_threshold\n",
    "    ):\n",
    "    \"\"\"Trains a Customer Lifetime Value model\"\"\"\n",
    "    \n",
    "    \n",
    "    engineer_features = bigquery_query_op(\n",
    "        query=feature_engineering_query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=features_dataset_id,\n",
    "        table_id=features_table_id)\n",
    "    \n",
    "    # Create an AML Dataset\n",
    "    create_dataset = automl_create_dataset_op(\n",
    "        project_id=project_id,\n",
    "        region=aml_compute_region,\n",
    "        display_name=aml_dataset_name\n",
    "    )\n",
    "    create_dataset.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    # Import the features from BigQuery to AML Dataset\n",
    "    table_uri = 'bq://{}.{}.{}'.format(project_id, features_dataset_id, features_table_id)\n",
    "    import_data = automl_import_data_from_bq_op(\n",
    "        dataset_name=create_dataset.output,\n",
    "        table_uri=table_uri\n",
    "    )\n",
    "    import_data.after(engineer_features)\n",
    "    \n",
    "    # Set the target column\n",
    "    set_target_column = automl_set_target_column_op(\n",
    "        dataset_name=import_data.output,\n",
    "        target_column_name=target_column_name\n",
    "    )\n",
    "    \n",
    "    # Create a model\n",
    "    create_model = automl_create_model__op(\n",
    "        project_id=project_id,\n",
    "        region=aml_compute_region,\n",
    "        display_name=aml_model_name,\n",
    "        dataset_name=set_target_column.output,\n",
    "        optimization_objective=optimization_objective,\n",
    "        train_budget_milli_node_hours=train_budget\n",
    "    )\n",
    "    \n",
    "    # Retrieve the primary metric from the model evaluations\n",
    "    log_regression_metrics = log_regression_metrics_op(create_model.output, primary_metric)\n",
    "    \n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(log_regression_metrics.outputs['primary_metric_value'] < deployment_threshold):\n",
    "        deploy_model = deploy_model_op(create_model.output)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_yaml = 'clv_training.yaml'\n",
    "kfp.compiler.Compiler().compile(pipeline_func=clv_train, package_path=pipeline_yaml, type_check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the pipeline to KFP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `kfp.Client()` to upload the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'clv_training_pipeline'\n",
    "client = kfp.Client(ENDPOINT)\n",
    "\n",
    "pipelines = [pipeline for pipeline in client.list_pipelines(page_size=100).pipelines if pipeline.name == pipeline_name]\n",
    "\n",
    "if pipelines:\n",
    "    print(\"Pipeline with this name already exists\")\n",
    "    pipeline_ref = pipelines[0]\n",
    "    \n",
    "else:\n",
    "    pipeline_ref = client.upload_pipeline(pipeline_yaml, pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'CLV Training'\n",
    "run_name = 'Run 01'\n",
    "params = dict(\n",
    "    project_id=PROJECT_ID\n",
    ")\n",
    "\n",
    "try:\n",
    "    experiment_ref = client.get_experiment(experiment_name)\n",
    "except:\n",
    "    experiment_ref = client.create_experiment(experiment_name)\n",
    "\n",
    "run_ref =client.run_pipeline(\n",
    "    experiment_ref.id,\n",
    "    run_name,\n",
    "    pipeline_package_path=None,\n",
    "    params=params,\n",
    "    pipeline_id=pipeline_ref.id)\n",
    "\n",
    "(run_ref.name, run_ref.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --project_id=$PROJECT_ID rm -r -f -d $DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
