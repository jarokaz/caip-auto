{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating AutoML Tables training and deployment with Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you develop a continous training and deployment pipeline using Kubeflow Pipelines, BigQuery and AutoML Tables.\n",
    "\n",
    "The scenario used in the lab is  predicting customer lifetime value (CLV).\n",
    "\n",
    "The goal of CLV modeling is to identify the most valuable customers - customers that are going to generate the highest value in a given future time range. The CLV models are built from a variety of data sources - historical sales data being the most important one and in many cases the only one. \n",
    "\n",
    "Predicting Customer Lifetime Value (CLV)  is a representative example of a use case where you may need to fine-tune and re-train a predictive model on a frequent basis. As there is a constant flow of new sales transactions that constitute the core of training data, models have to be kept up to date with evolving purchase patterns. Automation of model training and deployment is critical. \n",
    "\n",
    "In the CLV model developed in this lab, the historical sales transactions are preprocessed and aggregated to engineer a set of latent features  representing the so-called RFM characteristics of your customers:\n",
    "- Recency: How active have they been recently?\n",
    "- Frequency: How often do they buy?\n",
    "- Monetary: What amount do they spend?\n",
    "\n",
    "The following diagram shows a succession of past sales for a set of four customers.\n",
    "\n",
    "![clv_timeline](../../images/clv-timeline.png)\n",
    "\n",
    "The diagram illustrates the RFM values for the customers, showing for each customer:\n",
    "- Recency: The time between the last purchase and today, represented by the distance between the leftmost circle and the vertical dotted line that's labeled **Now**.\n",
    "- Frequency: The time between purchaes, represented by the distance between the circles on a single line.\n",
    "- Monetary: The amount of money spent on each purchase, represented by the size of the circle.\n",
    "\n",
    "As demonstrated in the lab you usually create multiple features per each characteristic. For example, in the lab, Recency is captured by two features: *recency* and *T*.\n",
    "\n",
    "The RFM input features and the target label are engineered using the following process:\n",
    "- A time series of of historical sales transactions for a given customer is divided into two time periods: *the features period* and *the predict period*. A point in time that is used to divide the time series is referred two as *the threshold date*. \n",
    "- The transactions in *the features period* are aggregated to create the latent RFM input features \n",
    "- The transactions in *the predict period* are aggregated to calculate the target label representing the expected total value of the customer\n",
    "\n",
    "This process results in a single example per customer and a set of examples across a customer population constitutes a training set.\n",
    "\n",
    "The pipeline implemented in the lab, uses BigQuery as a source of historical sales transactions. BigQuery is also used to engineer RFM features. The model is then trained and deployed using AutoML Tables. The below diagram represents the control and data flow implemented by the pipeline.\n",
    "\n",
    "\n",
    "![Training pipeline](../images/clv_train.png)\n",
    "\n",
    "1. The BQ query is run to process sales transactions in the *transactions* table into RFM features in the *features* table. \n",
    "1. The data from the *features* table is imported into the AutoML dataset\n",
    "1. The AutoML model is trained on the imported dataset\n",
    "1. After the training completes the evaluation metrics are retrieved and compared against the performance threshold\n",
    "1. If the model performs better than the threshold the model is deployed to AutoML Deployment\n",
    "\n",
    "The sample dataset used in the lab is based on the publicaly available [Online Retail Data Set](http://archive.ics.uci.edu/ml/datasets/Online+Retail) from the UCI Machine Learning Repository. The original dataset was preprocessed to conform to the following schema:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| customer_id | string | A unique customer ID |\n",
    "| order_date | date (yyyy-MM-dd) | The date of a transaction. Transactions (potentially from multiple invoices) are grouped by day |\n",
    "| quantity | integer | A number of items of a single SKU in a transaction |\n",
    "| unit_price | float | A unit price of a SKU |\n",
    "\n",
    "The feature engineering query generates the features table with the below schema. \n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| monetary | Float | The total spend by a customer in the features period|\n",
    "| frequency | Integer | The number of transactions placed by a customer in the features period |\n",
    "| recency | Integer |  The time (in days) between the first and the last orders in the features period |\n",
    "| T | Integer | The time between the first order placed and in the threshold date|\n",
    "| time_between | Float |  The average time betwee orders in the features period |\n",
    "| avg_basket_value | Float |  The averate monetary value of the customer's basket in the features period |\n",
    "| avg_basket_size | Float |  The average number of items in a basket in the features perio|\n",
    "| cnt_returns | Integer |  The number of returns in the features period|\n",
    "| target_monetary | Float | The total amount spent in the predict period. This is the label for predictions|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare lab environment\n",
    "Let's start with configuring your GCP environment settings and uploading the sales transactions into BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple\n",
    "from google.cloud import bigquery\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'mlops-dev-env'\n",
    "ENDPOINT = '55c0912bf09eab62-dot-us-central2.pipelines.googleusercontent.com'\n",
    "DATASET_ID = 'clv'\n",
    "TRANSACTIONS_TABLE_ID = 'transactions'\n",
    "TRANSACTIONS_TABLE_SCHEMA = 'customer_id:STRING,order_date:DATE,quantity:INTEGER,unit_price:FLOAT'\n",
    "TRANSACTIONS_SOURCE_FILE = '../../datasets/clv/transactions.csv'\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.5.1/components/gcp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a BigQuery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'mlops-dev-env:clv' successfully created.\n"
     ]
    }
   ],
   "source": [
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sale transactions data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload complete.\n",
      "Waiting on bqjob_r6ffc83fb23299aed_00000172e3822690_1 ... (3s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TRANSACTIONS_TABLE_ID \\\n",
    "$TRANSACTIONS_SOURCE_FILE \\\n",
    "$TRANSACTIONS_TABLE_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset\n",
    "To query data in BigQuery you can use BigQuery Python client library ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>256</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16701</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13941</td>\n",
       "      <td>2011-06-21</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14258</td>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18102</td>\n",
       "      <td>2011-07-28</td>\n",
       "      <td>256</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>256</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>256</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>256</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-08-11</td>\n",
       "      <td>256</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  order_date  quantity  unit_price\n",
       "0       14646  2011-05-12       256        1.65\n",
       "1       16553  2011-05-18       256        0.36\n",
       "2       16701  2011-05-18       256        0.36\n",
       "3       13941  2011-06-21       256        0.36\n",
       "4       14258  2011-07-11       256        0.36\n",
       "5       18102  2011-07-28       256        5.88\n",
       "6       14646  2011-08-09       256        0.72\n",
       "7       14646  2011-08-09       256        0.72\n",
       "8       14646  2011-08-09       256        0.72\n",
       "9       14646  2011-08-11       256        2.55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM `{{ source_table }}`\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TRANSACTIONS_TABLE_ID))\n",
    "\n",
    "df = client.query(query).to_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or Jupyter the `%%bigquery` magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-03-10</td>\n",
       "      <td>2</td>\n",
       "      <td>12.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-02-08</td>\n",
       "      <td>4</td>\n",
       "      <td>12.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-03-10</td>\n",
       "      <td>4</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>6</td>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-03-10</td>\n",
       "      <td>120</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-06-29</td>\n",
       "      <td>144</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>16553</td>\n",
       "      <td>2010-12-14</td>\n",
       "      <td>192</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-04-12</td>\n",
       "      <td>-4</td>\n",
       "      <td>12.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>16553</td>\n",
       "      <td>2010-12-08</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  order_date  quantity  unit_price\n",
       "0        16553  2011-05-18       256        0.36\n",
       "1        16553  2011-03-10         2       12.75\n",
       "2        16553  2011-02-08         4       12.75\n",
       "3        16553  2011-03-10         4        5.95\n",
       "4        16553  2011-05-18         6        2.95\n",
       "..         ...         ...       ...         ...\n",
       "81       16553  2011-03-10       120        0.85\n",
       "82       16553  2011-06-29       144        0.53\n",
       "83       16553  2010-12-14       192        0.85\n",
       "84       16553  2011-04-12        -4       12.75\n",
       "85       16553  2010-12-08        -1        4.25\n",
       "\n",
       "[86 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT_ID\n",
    "SELECT *\n",
    "FROM `clv.transactions`\n",
    "WHERE customer_id='16553'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are multiple sales transactions per customer. They represent the purchasing history and behavior of a given customer. For example, \n",
    "the customer identified by 16553 has 85 orders. Most of them are new purchases. Some of them are returns - the records with a negative quantity.\n",
    "\n",
    "The feature engineering query converts these 85 records into a single record representing the RFM charateristics of this customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the KFP training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create component factories for the pre-defined GCP components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None,\n",
    "    url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "    \n",
    "automl_create_dataset_op = component_store.load_component('automl/create_dataset_for_tables')\n",
    "automl_import_data_from_bq_op = component_store.load_component('automl/import_data_from_bigquery')\n",
    "automl_create_model__op = component_store.load_component('automl/create_model_for_tables')\n",
    "automl_split_dataset_table_column_names_op = component_store.load_component('automl/split_dataset_table_column_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base docker image for the custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.7\n",
    "RUN pip3 install --upgrade google-cloud-bigquery google-api-core google-cloud-automl grpcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 5 file(s) totalling 95.5 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://mlops-dev-env_cloudbuild/source/1592955058.17-36af6e282281480dbc07abb01611d831.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/mlops-dev-env/builds/8c0d2a8f-4c16-47c3-b729-482e5ea0d141].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/8c0d2a8f-4c16-47c3-b729-482e5ea0d141?project=881178567352].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"8c0d2a8f-4c16-47c3-b729-482e5ea0d141\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://mlops-dev-env_cloudbuild/source/1592955058.17-36af6e282281480dbc07abb01611d831.tgz#1592955058403369\n",
      "Copying gs://mlops-dev-env_cloudbuild/source/1592955058.17-36af6e282281480dbc07abb01611d831.tgz#1592955058403369...\n",
      "/ [1 files][ 23.2 KiB/ 23.2 KiB]                                                \n",
      "Operation completed over 1 objects/23.2 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "\n",
      "                   ***** NOTICE *****\n",
      "\n",
      "Alternative official `docker` images, including multiple versions across\n",
      "multiple platforms, are maintained by the Docker Team. For details, please\n",
      "visit https://hub.docker.com/_/docker.\n",
      "\n",
      "                ***** END OF NOTICE *****\n",
      "\n",
      "Sending build context to Docker daemon  103.4kB\n",
      "Step 1/2 : FROM python:3.7\n",
      "3.7: Pulling from library/python\n",
      "e9afc4f90ab0: Pulling fs layer\n",
      "989e6b19a265: Pulling fs layer\n",
      "af14b6c2f878: Pulling fs layer\n",
      "5573c4b30949: Pulling fs layer\n",
      "11a88e764313: Pulling fs layer\n",
      "ee776f0e36af: Pulling fs layer\n",
      "9075e1fb5d17: Pulling fs layer\n",
      "7882db2cc9d4: Pulling fs layer\n",
      "955a532dd46c: Pulling fs layer\n",
      "5573c4b30949: Waiting\n",
      "11a88e764313: Waiting\n",
      "ee776f0e36af: Waiting\n",
      "9075e1fb5d17: Waiting\n",
      "7882db2cc9d4: Waiting\n",
      "955a532dd46c: Waiting\n",
      "989e6b19a265: Verifying Checksum\n",
      "989e6b19a265: Download complete\n",
      "af14b6c2f878: Verifying Checksum\n",
      "af14b6c2f878: Download complete\n",
      "e9afc4f90ab0: Verifying Checksum\n",
      "e9afc4f90ab0: Download complete\n",
      "5573c4b30949: Verifying Checksum\n",
      "5573c4b30949: Download complete\n",
      "ee776f0e36af: Verifying Checksum\n",
      "ee776f0e36af: Download complete\n",
      "7882db2cc9d4: Verifying Checksum\n",
      "7882db2cc9d4: Download complete\n",
      "9075e1fb5d17: Verifying Checksum\n",
      "9075e1fb5d17: Download complete\n",
      "955a532dd46c: Verifying Checksum\n",
      "955a532dd46c: Download complete\n",
      "11a88e764313: Verifying Checksum\n",
      "11a88e764313: Download complete\n",
      "e9afc4f90ab0: Pull complete\n",
      "989e6b19a265: Pull complete\n",
      "af14b6c2f878: Pull complete\n",
      "5573c4b30949: Pull complete\n",
      "11a88e764313: Pull complete\n",
      "ee776f0e36af: Pull complete\n",
      "9075e1fb5d17: Pull complete\n",
      "7882db2cc9d4: Pull complete\n",
      "955a532dd46c: Pull complete\n",
      "Digest: sha256:f261d8fbee60f434341b7370fe0e5f3bea0deb0976ad323f0294d7dd694a9426\n",
      "Status: Downloaded newer image for python:3.7\n",
      " ---> e4e55e98f1e0\n",
      "Step 2/2 : RUN pip3 install --upgrade google-cloud-bigquery google-api-core google-cloud-automl grpcio\n",
      " ---> Running in 5e5178cdc8ba\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-1.25.0-py2.py3-none-any.whl (169 kB)\n",
      "Collecting google-api-core\n",
      "  Downloading google_api_core-1.21.0-py2.py3-none-any.whl (90 kB)\n",
      "Collecting google-cloud-automl\n",
      "  Downloading google_cloud_automl-1.0.1-py2.py3-none-any.whl (372 kB)\n",
      "Collecting grpcio\n",
      "  Downloading grpcio-1.30.0-cp37-cp37m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.12.2-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0\n",
      "  Downloading google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB)\n",
      "Collecting google-auth<2.0dev,>=1.9.0\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "Collecting six<2.0.0dev,>=1.13.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.1.0\n",
      "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in /usr/local/lib/python3.7/site-packages (from google-api-core) (47.1.1)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: six, protobuf, google-resumable-media, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, googleapis-common-protos, pytz, chardet, certifi, urllib3, idna, requests, google-api-core, google-cloud-core, google-cloud-bigquery, google-cloud-automl, grpcio\n",
      "Successfully installed cachetools-4.1.0 certifi-2020.6.20 chardet-3.0.4 google-api-core-1.21.0 google-auth-1.18.0 google-cloud-automl-1.0.1 google-cloud-bigquery-1.25.0 google-cloud-core-1.3.0 google-resumable-media-0.5.1 googleapis-common-protos-1.52.0 grpcio-1.30.0 idna-2.9 protobuf-3.12.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytz-2020.1 requests-2.24.0 rsa-4.6 six-1.15.0 urllib3-1.25.9\n",
      "Removing intermediate container 5e5178cdc8ba\n",
      " ---> 1bd4223de8aa\n",
      "Successfully built 1bd4223de8aa\n",
      "Successfully tagged gcr.io/mlops-dev-env/clv_components:latest\n",
      "PUSH\n",
      "Pushing gcr.io/mlops-dev-env/clv_components:latest\n",
      "\n",
      "                   ***** NOTICE *****\n",
      "\n",
      "Alternative official `docker` images, including multiple versions across\n",
      "multiple platforms, are maintained by the Docker Team. For details, please\n",
      "visit https://hub.docker.com/_/docker.\n",
      "\n",
      "                ***** END OF NOTICE *****\n",
      "\n",
      "The push refers to repository [gcr.io/mlops-dev-env/clv_components]\n",
      "7131838d49c3: Preparing\n",
      "f8d89ba0399b: Preparing\n",
      "2be8fac4a55f: Preparing\n",
      "dabd0b48256e: Preparing\n",
      "98d95bdfa037: Preparing\n",
      "da9418a2e1b1: Preparing\n",
      "2e5b4ca91984: Preparing\n",
      "527ade4639e0: Preparing\n",
      "c2c789d2d3c5: Preparing\n",
      "8803ef42039d: Preparing\n",
      "da9418a2e1b1: Waiting\n",
      "2e5b4ca91984: Waiting\n",
      "527ade4639e0: Waiting\n",
      "c2c789d2d3c5: Waiting\n",
      "8803ef42039d: Waiting\n",
      "f8d89ba0399b: Layer already exists\n",
      "98d95bdfa037: Layer already exists\n",
      "2be8fac4a55f: Layer already exists\n",
      "dabd0b48256e: Layer already exists\n",
      "da9418a2e1b1: Layer already exists\n",
      "527ade4639e0: Layer already exists\n",
      "2e5b4ca91984: Layer already exists\n",
      "c2c789d2d3c5: Layer already exists\n",
      "8803ef42039d: Layer already exists\n",
      "7131838d49c3: Pushed\n",
      "latest: digest: sha256:6f4b9ee9b8525f0e4ae76fb904a151f456445df994ea640d097555315b131773 size: 2429\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                   IMAGES                                         STATUS\n",
      "8c0d2a8f-4c16-47c3-b729-482e5ea0d141  2020-06-23T23:30:58+00:00  55S       gs://mlops-dev-env_cloudbuild/source/1592955058.17-36af6e282281480dbc07abb01611d831.tgz  gcr.io/mlops-dev-env/clv_components (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME=\"clv_components\"\n",
    "IMAGE_URI=\"gcr.io/{}/{}:latest\".format(PROJECT_ID, IMAGE_NAME)\n",
    "!gcloud builds submit --timeout 15m --tag $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create BQ query component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_query(query: str, \n",
    "             project_id:str, \n",
    "             dataset_id: str, \n",
    "             table_id: str, \n",
    "             location: str) -> NamedTuple('Outputs', [('table_uri', str), ('job_id', str)]):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.api_core import exceptions\n",
    "    import logging\n",
    "    import os\n",
    "    import uuid\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    client = bigquery.Client(project=project_id, location=location)\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "    job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_id = 'query_' + os.environ.get('KFP_POD_NAME', uuid.uuid1().hex)\n",
    "    \n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    try:\n",
    "        dataset = client.get_dataset(dataset_ref)\n",
    "    except exceptions.NotFound:\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = location\n",
    "        logging.info('Creating dataset {}'.format(dataset_id))\n",
    "        client.create_dataset(dataset)\n",
    "     \n",
    "    table_id = table_id if table_id else job_id\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    job_config.destination = table_ref\n",
    "    logging.info('Submitting the job {}'.format(job_id))\n",
    "    query_job = client.query(query, job_config, job_id=job_id)\n",
    "    query_job.result() # Wait for query to finish\n",
    "            \n",
    "    table_uri = 'bq://{}.{}.{}'.format(project_id, dataset_id, table_id)\n",
    "    \n",
    "    return (table_uri, job_id)\n",
    "    \n",
    "bq_query_op = func_to_container_op(bq_query, base_image='gcr.io/mlops-workshop/lab_11_components:latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a component that retrieves and logs AutoML evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_log_regression_metrics(model_path: str,\n",
    "               primary_metric:str) -> NamedTuple('Outputs', [('primary_metric', str), ('primary_metric_value', float)]):\n",
    "    \n",
    "    import logging\n",
    "    import json\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient()\n",
    "\n",
    "    # Retrieve evaluation metrics\n",
    "    for evaluation in client.list_model_evaluations(model_name=model_path):\n",
    "        if evaluation.regression_evaluation_metrics.ListFields():\n",
    "            evaluation_metrics = evaluation.regression_evaluation_metrics      \n",
    "    primary_metric_value = getattr(evaluation_metrics, primary_metric)\n",
    "    \n",
    "    # Write the primary metric as a KFP pipeline metric\n",
    "    metrics = {\n",
    "        'metrics': [{\n",
    "            'name': primary_metric.replace('_', '-'),\n",
    "            'numberValue': primary_metric_value\n",
    "        }]\n",
    "    }\n",
    "    with open('/mlpipeline-metrics.json', 'w') as f:\n",
    "       json.dump(metrics, f)\n",
    "    \n",
    "    return (primary_metric, primary_metric_value)\n",
    "    \n",
    "    \n",
    "log_regression_metrics_op = func_to_container_op(automl_log_regression_metrics, base_image='gcr.io/mlops-workshop/lab_11_components:latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a component that deploys an AutoML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_deploy_model(model_path: str):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    from google.cloud.automl_v1beta1 import enums\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient()\n",
    "    \n",
    "    model = client.get_model(model_name=model_path)\n",
    "    if model.deployment_state != enums.Model.DeploymentState.DEPLOYED:\n",
    "        logging.info(\"Starting model deployment: {}\".format(model_path))\n",
    "        response = client.deploy_model(model_name=model_path)\n",
    "        response.result() # Wait for operation to complete\n",
    "        logging.info(\"Deployment completed\")\n",
    "    else:\n",
    "         logging.info(\"Model already deployed\")\n",
    "    \n",
    "    \n",
    "deploy_model_op = func_to_container_op(automl_deploy_model, base_image='gcr.io/mlops-workshop/lab_11_components:latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline\n",
    "#### Set default values for pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = PROJECT_ID\n",
    "features_dataset_id = DATASET_ID\n",
    "features_dataset_location = DATASET_LOCATION\n",
    "features_table_id = 'features'\n",
    "aml_compute_region = 'us-central1'\n",
    "aml_dataset_name = 'clv_features'\n",
    "aml_model_name = 'clv_regression'\n",
    "target_column_name = 'target_monetary'\n",
    "train_budget = 1000\n",
    "optimization_objective = 'MINIMIZE_MAE'\n",
    "primary_metric = 'mean_absolute_error'\n",
    "deployment_threshold = 900\n",
    "\n",
    "# Read and render the query template\n",
    "query_template_file = 'query_template.sql.jinja'\n",
    "with open(query_template_file, 'r') as file:\n",
    "    query_template = file.read()\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    data_source_id='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TRANSACTIONS_TABLE_ID),\n",
    "    threshold_date='2011-08-08',\n",
    "    predict_end='2011-12-12',\n",
    "    max_monetary=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='CLV Training',\n",
    "    description='CLV Training Pipeline using BigQuery for feature engineering and Automl Tables for model training'\n",
    ")\n",
    "def clv_train(\n",
    "    project_id:str,\n",
    "    feature_engineering_query:str =query,\n",
    "    aml_compute_region:str =aml_compute_region,\n",
    "    features_table_id:str =features_table_id,\n",
    "    features_dataset_id:str =features_dataset_id,\n",
    "    features_dataset_location:str =features_dataset_location,\n",
    "    aml_dataset_name:str =aml_dataset_name,\n",
    "    target_column_name:str =target_column_name,\n",
    "    aml_model_name:str =aml_model_name,\n",
    "    train_budget:'Integer' =train_budget,\n",
    "    optimization_objective:str =optimization_objective,\n",
    "    primary_metric:str =primary_metric,\n",
    "    deployment_threshold:'Float' =deployment_threshold\n",
    "    ):\n",
    "    \"\"\"Trains a Customer Lifetime Value model\"\"\"\n",
    "    \n",
    "    # Use BigQuery to engineer features from transaction data\n",
    "    engineer_features = bq_query_op(\n",
    "        query=feature_engineering_query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=features_dataset_id,\n",
    "        table_id=features_table_id,\n",
    "        location=features_dataset_location)\n",
    "    \n",
    "    # Create an AML Dataset\n",
    "    create_dataset = automl_create_dataset_op(\n",
    "        gcp_project_id=project_id,\n",
    "        gcp_region=aml_compute_region,\n",
    "        display_name=aml_dataset_name\n",
    "    )\n",
    "    \n",
    "    # Import the features from BigQuery to AML Dataset\n",
    "    import_data = automl_import_data_from_bq_op(\n",
    "        dataset_path=create_dataset.outputs['dataset_path'],\n",
    "        input_uri=engineer_features.outputs['table_uri']\n",
    "    )\n",
    "    \n",
    "    # Set the target and feature columns\n",
    "    split_column_specs = automl_split_dataset_table_column_names_op(\n",
    "        dataset_path=import_data.outputs['dataset_path'],\n",
    "        table_index=0,\n",
    "        target_column_name=target_column_name\n",
    "    )\n",
    "    \n",
    "    # Create a model\n",
    "    create_model = automl_create_model__op(\n",
    "        gcp_project_id=project_id,\n",
    "        gcp_region=aml_compute_region,\n",
    "        display_name=aml_model_name,\n",
    "        dataset_id=create_dataset.outputs['dataset_id'],\n",
    "        target_column_path=split_column_specs.outputs['target_column_path'],\n",
    "        input_feature_column_paths=split_column_specs.outputs['feature_column_paths'],\n",
    "        optimization_objective=optimization_objective,\n",
    "        train_budget_milli_node_hours=train_budget\n",
    "    )\n",
    "    \n",
    "    # Retrieve the primary metric from the model evaluations\n",
    "    log_regression_metrics = log_regression_metrics_op(create_model.outputs['model_path'], primary_metric)\n",
    "    \n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(log_regression_metrics.outputs['primary_metric_value'] < deployment_threshold):\n",
    "        deploy_model = deploy_model_op(create_model.outputs['model_path'])\n",
    "    \n",
    "    from kfp.gcp import use_gcp_secret\n",
    "    kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_yaml = 'clv_training.yaml'\n",
    "kfp.compiler.Compiler().compile(clv_train, pipeline_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the pipeline to KFP\n",
    "Get GKE cluster credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for mlops-workshop-cluster.\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT_ID\n",
    "!gcloud container clusters get-credentials $CLUSTER_NAME --zone $CLUSTER_ZONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `kfp.Client()` to upload the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Pipeline link <a href=/pipeline/#/pipelines/details/d86958ad-2b1f-4e47-9523-b7dfe0303cd2>here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_name = 'clv_training_pipeline'\n",
    "client = kfp.Client()\n",
    "\n",
    "pipelines = [pipeline for pipeline in client.list_pipelines(page_size=100).pipelines if pipeline.name == pipeline_name]\n",
    "\n",
    "if pipelines:\n",
    "    print(\"Pipeline with this name already exists\")\n",
    "    pipeline_ref = pipelines[0]\n",
    "    \n",
    "else:\n",
    "    pipeline_ref = client.upload_pipeline(pipeline_yaml, pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/e521abdc-fa14-42e2-b4f3-2c7b4e86de60\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/a7c96154-e7d7-4e9e-a8de-5bc5bd1f456e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Run 01', 'a7c96154-e7d7-4e9e-a8de-5bc5bd1f456e')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = 'CLV Training'\n",
    "run_name = 'Run 01'\n",
    "params = dict(\n",
    "    project_id=PROJECT_ID\n",
    ")\n",
    "\n",
    "try:\n",
    "    experiment_ref = client.get_experiment(experiment_name)\n",
    "except:\n",
    "    experiment_ref = client.create_experiment(experiment_name)\n",
    "\n",
    "run_ref =client.run_pipeline(\n",
    "    experiment_ref.id,\n",
    "    run_name,\n",
    "    pipeline_package_path=None,\n",
    "    params=params,\n",
    "    pipeline_id=pipeline_ref.id)\n",
    "\n",
    "(run_ref.name, run_ref.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --project_id=$PROJECT_ID rm -r -f -d $DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
